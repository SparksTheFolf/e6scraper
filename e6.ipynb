{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSG\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time\n",
    "\n",
    "BASE_URL = 'https://e621.net/'\n",
    "RATE_TIME = 1.5\n",
    "\n",
    "class E621WrapperError(Exception):\n",
    "    pass\n",
    "\n",
    "class E621Wrapper:\n",
    "    def __init__(self, username: str, api_key: str, user_agent: str = None, wait_for_rate_limit: bool = True) -> None:\n",
    "        if not user_agent:\n",
    "            self.user_agent = f'e621_scraper (by {username} on e621)'\n",
    "        self.user_agent = str(user_agent)\n",
    "\n",
    "        self._wait_for_rate_limit = wait_for_rate_limit\n",
    "        self._last_requested = 0\n",
    "\n",
    "        self._wrapper_session = requests.Session()\n",
    "        self._wrapper_session.auth = HTTPBasicAuth(username, api_key)\n",
    "        self._wrapper_session.headers.update({'User-Agent' : user_agent})\n",
    "\n",
    "    def _wait(self) -> None:\n",
    "        if self._wait_for_rate_limit:\n",
    "            while True:\n",
    "                if time.time() - self._last_requested > RATE_TIME:\n",
    "                    return\n",
    "    \n",
    "\n",
    "    def get_posts(self, limit: int = 10, tags: list = [], page: str = None) -> dict:\n",
    "        route = BASE_URL + 'posts.json'\n",
    "        param_builder = {}\n",
    "        \n",
    "        if limit > 320:\n",
    "            limit = 320\n",
    "        param_builder['limit'] = limit\n",
    "\n",
    "        tag_str = ' '.join(tags)\n",
    "        param_builder['tags'] = tag_str\n",
    "\n",
    "        if page:\n",
    "            param_builder['page'] = page\n",
    "\n",
    "        self._wait()\n",
    "        self._last_requested = time.time()\n",
    "        r = self._wrapper_session.get(route, params=param_builder)\n",
    "\n",
    "        return r.json()['posts']\n",
    "    \n",
    "    def get_tags(self, limit: int = 75, category: int = None, page: str = None) -> dict:\n",
    "        route = BASE_URL + 'tags.json'\n",
    "        param_builder = {}\n",
    "        \n",
    "        if limit > 320:\n",
    "            limit = 320\n",
    "        param_builder['limit'] = limit\n",
    "\n",
    "        if type(category) != type(int()):\n",
    "            raise ValueError('Category must be type int()')\n",
    "\n",
    "        if 0 <= category <= 8:\n",
    "            param_builder['search[category]'] = category\n",
    "        if page:\n",
    "            param_builder['page'] = page\n",
    "\n",
    "        self._wait()\n",
    "        self._last_requested = time.time()\n",
    "        r = self._wrapper_session.get(route, params=param_builder)\n",
    "\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect Posts\n",
    "from monosodium_glutamate import E621Wrapper\n",
    "import json\n",
    "\n",
    "USERNAME = 'ENTERUSER'\n",
    "API_KEY = 'ENTERAPIKEY'\n",
    "\n",
    "DATA_FILE = 'data/posts.json'\n",
    "\n",
    "def convert_rating(rating: str) -> list:\n",
    "    if rating == 's':\n",
    "        return [1, 0, 0]\n",
    "    if rating == 'e':\n",
    "        return [0, 1, 0]\n",
    "    return [0, 0, 1]\n",
    "\n",
    "def load_posts() -> dict:\n",
    "    try:\n",
    "        with open(DATA_FILE, encoding='utf-8') as json_file:\n",
    "            return json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_posts(posts: dict) -> None:\n",
    "    print(f'saving {len(posts)} posts')\n",
    "    with open(DATA_FILE, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(posts, json_file, indent=2,ensure_ascii=False)\n",
    "\n",
    "def get_last_post() -> str:\n",
    "    try:\n",
    "        with open(DATA_FILE, encoding='utf-8') as json_file:\n",
    "            posts = json.load(json_file)\n",
    "            keys = posts.keys()\n",
    "            return 'b' + str(list(keys)[-1])\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "with open('key.json') as json_file:\n",
    "    key = json.load(json_file)[\"key\"]\n",
    "\n",
    "e = E621Wrapper(USERNAME, key, API_KEY)\n",
    "\n",
    "tags = [\"-animated\", \"-webm\", \"-flash\", \"-3d_(artwork)\", \"-sketch\", \"-pixel_(artwork)\", \"~digital_media_(artwork)\", \"~traditional_media_(artwork)\"]\n",
    "print(' '.join(tags))\n",
    "print(get_last_post())\n",
    "last_post = get_last_post()\n",
    "all_posts = load_posts()\n",
    "save_count = 0\n",
    "while len(all_posts) < 3200:\n",
    "    posts = e.get_posts(limit=320, tags=tags, page=last_post)\n",
    "    if len(posts) == 0:\n",
    "        break\n",
    "    \n",
    "    last_post = 'b' + str(posts[-1][\"id\"])\n",
    "    print(last_post)\n",
    "\n",
    "    for post in posts:\n",
    "        post_id = post['id']\n",
    "        url = post['file']['url']\n",
    "        ext = post['file']['ext']\n",
    "        rating = post['rating']\n",
    "        species = post['tags']['species']\n",
    "        general = post['tags']['general']\n",
    "        score = post['score']['total']\n",
    "        if ext in ['png', 'jpg']:\n",
    "            all_posts[str(post_id)] = {\n",
    "                \"url\"     : url,\n",
    "                \"rating\"  : convert_rating(rating),\n",
    "                \"score\"   : score,\n",
    "                \"species\" : species,\n",
    "                \"general\" : general\n",
    "                }\n",
    "        \n",
    "    save_count += 1\n",
    "    if save_count % 10 == 0:\n",
    "        save_posts(all_posts)\n",
    "\n",
    "save_posts(all_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get images\n",
    "import json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "ML_SIZE   = (512,512)\n",
    "DATA_FILE = 'data/posts.json'\n",
    "\n",
    "def load_posts() -> dict:\n",
    "    try:\n",
    "        with open(DATA_FILE, encoding='utf-8') as json_file:\n",
    "            return json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def create_image(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Failed to collect image. ({response.status_code})')\n",
    "        return None\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.convert('RGB')\n",
    "    img1 = img.resize(ML_SIZE)\n",
    "    return img1\n",
    "\n",
    "posts = load_posts()\n",
    "\n",
    "for item in list(posts.items()):\n",
    "    url = item[1]['url']\n",
    "    ext = url[url.rfind(\".\"):]\n",
    "    print(f'creating image {item[0]}.png')\n",
    "    print(url)\n",
    "    image = create_image(item[1]['url'])\n",
    "    if image:\n",
    "        image.save(f'images/{item[0]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Datasets\n",
    "import json\n",
    "import os \n",
    "\n",
    "DATA_FILE = 'data/posts.json'\n",
    "TAG_FILE  = 'data/tags_list.json'\n",
    "\n",
    "def load_posts_saved() -> list:\n",
    "    posts = os.listdir('images/')\n",
    "    posts = [p[:p.rfind('.')] for p in posts]\n",
    "    return posts\n",
    "\n",
    "def load_posts_dict() -> dict:\n",
    "    try:\n",
    "        with open(DATA_FILE, encoding='utf-8') as json_file:\n",
    "            return json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def load_tags() -> dict:\n",
    "    with open(TAG_FILE, encoding='utf-8') as json_file:\n",
    "        tags = json.load(json_file)\n",
    "        tags['species'] = list(tags['species'])\n",
    "        tags['general'] = list(tags['general'])\n",
    "        return tags\n",
    "\n",
    "def create_onehot(length, positions):\n",
    "    l = [0] * length\n",
    "    for pos in positions:\n",
    "        l[pos] = 1\n",
    "    return l\n",
    "\n",
    "tags = load_tags()\n",
    "species_len = len(tags['species'])\n",
    "general_len = len(tags['general'])\n",
    "\n",
    "posts = load_posts_saved()\n",
    "post_dict = load_posts_dict()\n",
    "\n",
    "s_header = f\"id, {', '.join(tags['species'])}\\n\"\n",
    "g_header = f\"id, {', '.join(tags['general'])}\\n\"\n",
    "\n",
    "s_lines = [s_header]\n",
    "g_lines = [g_header]\n",
    "\n",
    "for post in posts:\n",
    "    species_intersect = set(post_dict[post]['species']).intersection(tags['species'])\n",
    "    s_positions = [tags['species'].index(s) for s in species_intersect]\n",
    "    s_onehot = create_onehot(species_len, s_positions)\n",
    "\n",
    "    general_intersect = set(post_dict[post]['general']).intersection(tags['general'])\n",
    "    g_positions = [tags['general'].index(g) for g in general_intersect]\n",
    "    g_onehot = create_onehot(general_len, g_positions)\n",
    "    \n",
    "    s_line = f\"{post}, {', '.join(map(str, s_onehot))}\\n\"\n",
    "    s_lines.append(s_line)\n",
    "\n",
    "    g_line = f\"{post}, {', '.join(map(str, g_onehot))}\\n\"\n",
    "    g_lines.append(g_line)\n",
    "\n",
    "\n",
    "with open('data/species_dataset.csv', 'w', encoding='utf-8') as csv_file:\n",
    "    csv_file.writelines(s_lines)\n",
    "\n",
    "\n",
    "with open('data/general_dataset.csv', 'w', encoding='utf-8') as csv_file:\n",
    "    csv_file.writelines(g_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort them tags\n",
    "import json\n",
    "\n",
    "with open('data\\general_tags.json') as json_file:\n",
    "    general = list(json.load(json_file)[\"tags\"])\n",
    "\n",
    "with open('data/species_tags.json') as json_file:\n",
    "    species = list(json.load(json_file)[\"tags\"])\n",
    "\n",
    "CUTOFF = 7000\n",
    "\n",
    "species.sort(key= lambda x: x['post_count'], reverse=True)\n",
    "species = [s for s in species if s['post_count'] > CUTOFF]\n",
    "general.sort(key= lambda x: x['post_count'], reverse=True)\n",
    "general = [g for g in general if g['post_count'] > CUTOFF]\n",
    "\n",
    "tags = {\"species\": [s['name'] for s in species], \"general\": [g['name'] for g in general]}\n",
    "\n",
    "with open('data/tags_list.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(tags, json_file, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3ba4884712b2fd37087f4be8b9b395bb908997ee20f2eb6112d325f4c89f1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
